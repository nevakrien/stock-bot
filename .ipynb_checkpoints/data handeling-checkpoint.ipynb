{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84595fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import yahoo_fin as yf\n",
    "from yahoo_fin.stock_info import get_data\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from yahoo_fin import stock_info as si\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afa2b0",
   "metadata": {},
   "source": [
    "# getting data\n",
    "\n",
    "things we did to clean data:\n",
    "\n",
    "1. shamelessly copy:\n",
    "https://levelup.gitconnected.com/how-to-get-all-stock-symbols-a73925c16a1b\n",
    "\n",
    "2. we removed the tickers with $\n",
    "\n",
    "3. we added exception handeling that tries similer strings to the input (note that this is not implemented on the set itself but when fetching, leading to multiple requsts and potentialy to errors)\n",
    "4.  we just drop exceptions that are found when getting the data from online  (these are diffrent every day)\n",
    "\n",
    "# how are we keeping consistency? \n",
    "since the avilble stocks may change depending on the source of the data some extra steps need to be done in order to\n",
    "alow saves to work seemlesly \n",
    "\n",
    "\n",
    "basic plan:\n",
    "\n",
    "1. clean the data as much as possible \n",
    "\n",
    "2. save the embedings weights with their ticker string\n",
    "\n",
    "3. backup everything on git to alow us to fix it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0bece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame( si.tickers_sp500() )\n",
    "df2 = pd.DataFrame( si.tickers_nasdaq() )\n",
    "df3 = pd.DataFrame( si.tickers_dow() )\n",
    "df4 = pd.DataFrame( si.tickers_other() )\n",
    "\n",
    "sym1 = set( symbol for symbol in df1[0].values.tolist() )\n",
    "sym2 = set( symbol for symbol in df2[0].values.tolist() )\n",
    "sym3 = set( symbol for symbol in df3[0].values.tolist() )\n",
    "sym4 = set( symbol for symbol in df4[0].values.tolist() )\n",
    "\n",
    "symbols = set.union( sym1, sym2, sym3, sym4 )\n",
    "del(sym1, sym2, sym3, sym4,df1, df2, df3, df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469a803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1584 unqualified stock symbols...\n",
      "There are 10628 qualified stock symbols...\n"
     ]
    }
   ],
   "source": [
    "my_list = ['W', 'R', 'P', 'Q']\n",
    "del_set = set()\n",
    "sav_set = set()\n",
    "\n",
    "for symbol in symbols:\n",
    "    if (len( symbol ) > 4 and symbol[-1] in my_list or \"$\" in symbol):\n",
    "        del_set.add( symbol )\n",
    "    else:\n",
    "        sav_set.add( symbol )\n",
    "sav_set.discard(\"\") \n",
    "\n",
    "ticker_list=list(sav_set)\n",
    "\n",
    "print( f'Removed {len( del_set )} unqualified stock symbols...' )\n",
    "print( f'There are {len( sav_set )} qualified stock symbols...' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d42362",
   "metadata": {},
   "outputs": [],
   "source": [
    "start='01/01/2020' \n",
    "end='09/14/2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3c39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures \n",
    "\n",
    "def get_seiries(ticker,start,end,entry=\"adjclose\"):\n",
    "    try:\n",
    "        raw = get_data(ticker.split(\".\")[0], start_date=start, end_date=end)[entry]\n",
    "    except: \n",
    "        try:\n",
    "            raw = get_data(ticker.replace(\".\",\"-\"), start_date=start, end_date=end)[entry]\n",
    "        except:\n",
    "            raw = get_data(ticker[0:4], start_date=start, end_date=end)[entry]\n",
    "    return raw/raw[0]\n",
    "\n",
    "def get_df(l,start,end,entry=\"adjclose\"):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        x=[executor.submit(lambda x: get_seiries(x,start,end,entry=entry),name)for name in l]\n",
    "        x=[a.result() for a in x]\n",
    "    for i,c in enumerate(x):\n",
    "        c.name=l[i]\n",
    "    return pd.DataFrame(x).transpose()\n",
    "\n",
    "def tester_func(name,start=start,end=end,entry=\"adjclose\"):\n",
    "    try:\n",
    "        c= get_seiries(name,start,end,entry)\n",
    "        c.name=name\n",
    "        return (True,c)\n",
    "    except:\n",
    "        return (False,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586c18bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread setup complete\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "error number 1: name=KDIV\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "error number 2: name=OAIM\n",
      "800\n",
      "error number 3: name=CTEST.O\n",
      "error number 4: name=EAI\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "error number 5: name=ZBZX\n",
      "1600\n",
      "error number 6: name=CTEST.E\n",
      "1700\n",
      "error number 7: name=ZJZZT\n",
      "error number 8: name=XTWY\n",
      "1800\n",
      "error number 9: name=ZXIET\n",
      "error number 10: name=ATEST.B\n",
      "1900\n",
      "2000\n",
      "error number 11: name=RCA\n",
      "2100\n",
      "error number 12: name=ZVZZT\n",
      "error number 13: name=LNKB\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "error number 14: name=CBX\n",
      "error number 15: name=GGLS\n",
      "error number 16: name=ATEST\n",
      "2700\n",
      "error number 17: name=IBO\n",
      "2800\n",
      "error number 18: name=ZBZZT\n",
      "2900\n",
      "error number 19: name=ZAZZT\n",
      "3000\n",
      "error number 20: name=ZCZZT\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "error number 21: name=ATEST.A\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "error number 22: name=ZEXIT\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "error number 23: name=DHCNL\n",
      "4100\n",
      "error number 24: name=ZXZZT\n",
      "4200\n",
      "error number 25: name=CBO\n",
      "4300\n",
      "error number 26: name=IBIT\n",
      "4400\n",
      "error number 27: name=ZXYZ.A\n",
      "4500\n",
      "4600\n",
      "error number 28: name=XTRE\n",
      "4700\n",
      "error number 29: name=XTEN\n",
      "error number 30: name=ZVZZC\n",
      "4800\n",
      "error number 31: name=CTEST.V\n",
      "error number 32: name=CTEST.G\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "error number 33: name=ZWZZT\n",
      "5200\n",
      "error number 34: name=NXL\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "error number 35: name=XHLF\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "error number 36: name=ZVV\n",
      "6300\n",
      "6400\n",
      "error number 37: name=FOXO\n",
      "6500\n",
      "error number 38: name=ATEST.C\n",
      "error number 39: name=XFIV\n",
      "6600\n",
      "error number 40: name=CTEST\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "error number 41: name=XTWO\n",
      "7100\n",
      "error number 42: name=THRD\n",
      "7200\n",
      "error number 43: name=OAEM\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "error number 44: name=TAFI\n",
      "7600\n",
      "error number 45: name=IGZ\n",
      "7700\n",
      "7800\n",
      "error number 46: name=CTEST.S\n",
      "7900\n",
      "error number 47: name=STRV\n",
      "8000\n",
      "error number 48: name=SFB\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "error number 49: name=CRBG\n",
      "error number 50: name=GRP.U\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "error number 51: name=YEAR\n",
      "8800\n",
      "error number 52: name=ZIEXT\n",
      "8900\n",
      "9000\n",
      "error number 53: name=DEFI\n",
      "9100\n",
      "error number 54: name=XSVN\n",
      "error number 55: name=CTEST.L\n",
      "9200\n",
      "error number 56: name=XONE\n",
      "9300\n",
      "error number 57: name=EMP\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "error number 58: name=ZTEST\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "error number 59: name=AMPX\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "mini=ticker_list\n",
    "errors=[]\n",
    "data=[]\n",
    "c=0\n",
    "x=list(range(len(mini)))\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for i in range(len(mini)):\n",
    "        x[i]=executor.submit(tester_func,mini[i])\n",
    "    \n",
    "    print(\"thread setup complete\")\n",
    "    \n",
    "    for i in range(len(mini)):\n",
    "        if(i%100==0):\n",
    "            print(i)\n",
    "        r=x[i].result()\n",
    "        if (r[0]): \n",
    "            data.append(r[1])\n",
    "        else:\n",
    "            errors.append(r[1])\n",
    "            c+=1\n",
    "            print(f\"error number {c}: name={errors[-1]}\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e95aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRES</th>\n",
       "      <th>ONTO</th>\n",
       "      <th>NOGN</th>\n",
       "      <th>COWNL</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>HEPS</th>\n",
       "      <th>DXJ</th>\n",
       "      <th>FRBN</th>\n",
       "      <th>REX</th>\n",
       "      <th>PR</th>\n",
       "      <th>...</th>\n",
       "      <th>GOOD</th>\n",
       "      <th>IGC</th>\n",
       "      <th>OPFI</th>\n",
       "      <th>JPS</th>\n",
       "      <th>CNDT</th>\n",
       "      <th>APYX</th>\n",
       "      <th>KRBN</th>\n",
       "      <th>GCV</th>\n",
       "      <th>FARM</th>\n",
       "      <th>WBIG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>0.995231</td>\n",
       "      <td>0.985957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000731</td>\n",
       "      <td>0.995295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.003268</td>\n",
       "      <td>1.021322</td>\n",
       "      <td>...</td>\n",
       "      <td>1.004701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.004004</td>\n",
       "      <td>0.987603</td>\n",
       "      <td>0.995300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.008532</td>\n",
       "      <td>0.992476</td>\n",
       "      <td>0.993623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>0.997065</td>\n",
       "      <td>0.970564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000548</td>\n",
       "      <td>0.996727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.990241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993101</td>\n",
       "      <td>1.068230</td>\n",
       "      <td>...</td>\n",
       "      <td>1.004231</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.001001</td>\n",
       "      <td>0.923967</td>\n",
       "      <td>0.974148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.005120</td>\n",
       "      <td>0.996580</td>\n",
       "      <td>0.992526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07</th>\n",
       "      <td>0.993397</td>\n",
       "      <td>0.976236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.996894</td>\n",
       "      <td>0.996216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.990425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.962479</td>\n",
       "      <td>1.066098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989657</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.004004</td>\n",
       "      <td>0.945454</td>\n",
       "      <td>0.971798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.006826</td>\n",
       "      <td>0.967852</td>\n",
       "      <td>0.989643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>0.989362</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.002558</td>\n",
       "      <td>0.999489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.995949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.935004</td>\n",
       "      <td>0.982942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991067</td>\n",
       "      <td>1.015873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.004004</td>\n",
       "      <td>0.950413</td>\n",
       "      <td>0.967097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.006826</td>\n",
       "      <td>0.941861</td>\n",
       "      <td>0.993095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-07</th>\n",
       "      <td>1.259090</td>\n",
       "      <td>1.821766</td>\n",
       "      <td>0.202832</td>\n",
       "      <td>1.134360</td>\n",
       "      <td>1.242030</td>\n",
       "      <td>0.073716</td>\n",
       "      <td>1.268203</td>\n",
       "      <td>1.022312</td>\n",
       "      <td>1.027596</td>\n",
       "      <td>1.611940</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018092</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.265990</td>\n",
       "      <td>0.841365</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>1.948549</td>\n",
       "      <td>1.136384</td>\n",
       "      <td>0.343365</td>\n",
       "      <td>1.064245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-08</th>\n",
       "      <td>1.262720</td>\n",
       "      <td>1.890359</td>\n",
       "      <td>0.171704</td>\n",
       "      <td>1.133779</td>\n",
       "      <td>1.254682</td>\n",
       "      <td>0.077439</td>\n",
       "      <td>1.286874</td>\n",
       "      <td>1.022312</td>\n",
       "      <td>1.047930</td>\n",
       "      <td>1.656716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019169</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.265990</td>\n",
       "      <td>0.835586</td>\n",
       "      <td>0.659504</td>\n",
       "      <td>0.707403</td>\n",
       "      <td>1.895037</td>\n",
       "      <td>1.124165</td>\n",
       "      <td>0.348153</td>\n",
       "      <td>1.064783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-09</th>\n",
       "      <td>1.298613</td>\n",
       "      <td>1.930597</td>\n",
       "      <td>0.156642</td>\n",
       "      <td>1.139096</td>\n",
       "      <td>1.276690</td>\n",
       "      <td>0.078183</td>\n",
       "      <td>1.297093</td>\n",
       "      <td>1.022312</td>\n",
       "      <td>1.062455</td>\n",
       "      <td>1.692964</td>\n",
       "      <td>...</td>\n",
       "      <td>1.027783</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.268020</td>\n",
       "      <td>0.836742</td>\n",
       "      <td>0.674380</td>\n",
       "      <td>0.735605</td>\n",
       "      <td>1.894055</td>\n",
       "      <td>1.138421</td>\n",
       "      <td>0.374829</td>\n",
       "      <td>1.073552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12</th>\n",
       "      <td>1.311922</td>\n",
       "      <td>1.944910</td>\n",
       "      <td>0.151622</td>\n",
       "      <td>1.133466</td>\n",
       "      <td>1.287960</td>\n",
       "      <td>0.083395</td>\n",
       "      <td>1.295914</td>\n",
       "      <td>1.022312</td>\n",
       "      <td>1.064633</td>\n",
       "      <td>1.752665</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036936</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.275127</td>\n",
       "      <td>0.841365</td>\n",
       "      <td>0.692562</td>\n",
       "      <td>0.781434</td>\n",
       "      <td>2.017772</td>\n",
       "      <td>1.142494</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>1.074628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-13</th>\n",
       "      <td>1.270382</td>\n",
       "      <td>1.906562</td>\n",
       "      <td>0.149613</td>\n",
       "      <td>1.125871</td>\n",
       "      <td>1.237671</td>\n",
       "      <td>0.079672</td>\n",
       "      <td>1.272134</td>\n",
       "      <td>1.021298</td>\n",
       "      <td>1.025781</td>\n",
       "      <td>1.665245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998710</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.256853</td>\n",
       "      <td>0.833275</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.727380</td>\n",
       "      <td>1.946095</td>\n",
       "      <td>1.109909</td>\n",
       "      <td>0.370041</td>\n",
       "      <td>1.055890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>680 rows × 10569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                GRES      ONTO      NOGN     COWNL      SIZE      HEPS  \\\n",
       "2020-01-02  1.000000  1.000000       NaN  1.000000  1.000000       NaN   \n",
       "2020-01-03  0.995231  0.985957       NaN  1.000731  0.995295       NaN   \n",
       "2020-01-06  0.997065  0.970564       NaN  1.000548  0.996727       NaN   \n",
       "2020-01-07  0.993397  0.976236       NaN  0.996894  0.996216       NaN   \n",
       "2020-01-08  0.989362  0.992709       NaN  1.002558  0.999489       NaN   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-09-07  1.259090  1.821766  0.202832  1.134360  1.242030  0.073716   \n",
       "2022-09-08  1.262720  1.890359  0.171704  1.133779  1.254682  0.077439   \n",
       "2022-09-09  1.298613  1.930597  0.156642  1.139096  1.276690  0.078183   \n",
       "2022-09-12  1.311922  1.944910  0.151622  1.133466  1.287960  0.083395   \n",
       "2022-09-13  1.270382  1.906562  0.149613  1.125871  1.237671  0.079672   \n",
       "\n",
       "                 DXJ      FRBN       REX        PR  ...      GOOD       IGC  \\\n",
       "2020-01-02  1.000000       NaN  1.000000  1.000000  ...  1.000000  1.000000   \n",
       "2020-01-03  0.982692       NaN  1.003268  1.021322  ...  1.004701  1.000000   \n",
       "2020-01-06  0.990241       NaN  0.993101  1.068230  ...  1.004231  0.984127   \n",
       "2020-01-07  0.990425       NaN  0.962479  1.066098  ...  0.989657  0.984127   \n",
       "2020-01-08  0.995949       NaN  0.935004  0.982942  ...  0.991067  1.015873   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2022-09-07  1.268203  1.022312  1.027596  1.611940  ...  1.018092  0.873016   \n",
       "2022-09-08  1.286874  1.022312  1.047930  1.656716  ...  1.019169  0.888889   \n",
       "2022-09-09  1.297093  1.022312  1.062455  1.692964  ...  1.027783  0.873016   \n",
       "2022-09-12  1.295914  1.022312  1.064633  1.752665  ...  1.036936  0.920635   \n",
       "2022-09-13  1.272134  1.021298  1.025781  1.665245  ...  0.998710  0.904762   \n",
       "\n",
       "                OPFI       JPS      CNDT      APYX      KRBN       GCV  \\\n",
       "2020-01-02       NaN  1.000000  1.000000  1.000000       NaN  1.000000   \n",
       "2020-01-03       NaN  1.004004  0.987603  0.995300       NaN  1.008532   \n",
       "2020-01-06       NaN  1.001001  0.923967  0.974148       NaN  1.005120   \n",
       "2020-01-07       NaN  1.004004  0.945454  0.971798       NaN  1.006826   \n",
       "2020-01-08       NaN  1.004004  0.950413  0.967097       NaN  1.006826   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-09-07  0.265990  0.841365  0.669421  0.702703  1.948549  1.136384   \n",
       "2022-09-08  0.265990  0.835586  0.659504  0.707403  1.895037  1.124165   \n",
       "2022-09-09  0.268020  0.836742  0.674380  0.735605  1.894055  1.138421   \n",
       "2022-09-12  0.275127  0.841365  0.692562  0.781434  2.017772  1.142494   \n",
       "2022-09-13  0.256853  0.833275  0.669421  0.727380  1.946095  1.109909   \n",
       "\n",
       "                FARM      WBIG  \n",
       "2020-01-02  1.000000  1.000000  \n",
       "2020-01-03  0.992476  0.993623  \n",
       "2020-01-06  0.996580  0.992526  \n",
       "2020-01-07  0.967852  0.989643  \n",
       "2020-01-08  0.941861  0.993095  \n",
       "...              ...       ...  \n",
       "2022-09-07  0.343365  1.064245  \n",
       "2022-09-08  0.348153  1.064783  \n",
       "2022-09-09  0.374829  1.073552  \n",
       "2022-09-12  0.372093  1.074628  \n",
       "2022-09-13  0.370041  1.055890  \n",
       "\n",
       "[680 rows x 10569 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df=pd.DataFrame(data).transpose()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54693bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRES</th>\n",
       "      <th>ONTO</th>\n",
       "      <th>NOGN</th>\n",
       "      <th>COWNL</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>HEPS</th>\n",
       "      <th>DXJ</th>\n",
       "      <th>FRBN</th>\n",
       "      <th>REX</th>\n",
       "      <th>PR</th>\n",
       "      <th>...</th>\n",
       "      <th>GOOD</th>\n",
       "      <th>IGC</th>\n",
       "      <th>OPFI</th>\n",
       "      <th>JPS</th>\n",
       "      <th>CNDT</th>\n",
       "      <th>APYX</th>\n",
       "      <th>KRBN</th>\n",
       "      <th>GCV</th>\n",
       "      <th>FARM</th>\n",
       "      <th>WBIG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-06-07</th>\n",
       "      <td>1.452672</td>\n",
       "      <td>2.259519</td>\n",
       "      <td>1.006125</td>\n",
       "      <td>1.133254</td>\n",
       "      <td>1.303582</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>1.295589</td>\n",
       "      <td>1.006085</td>\n",
       "      <td>1.149843</td>\n",
       "      <td>2.040512</td>\n",
       "      <td>...</td>\n",
       "      <td>1.081622</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.322843</td>\n",
       "      <td>0.882971</td>\n",
       "      <td>0.912397</td>\n",
       "      <td>0.853114</td>\n",
       "      <td>2.352104</td>\n",
       "      <td>1.213772</td>\n",
       "      <td>0.350889</td>\n",
       "      <td>1.125753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-08</th>\n",
       "      <td>1.437347</td>\n",
       "      <td>2.178774</td>\n",
       "      <td>1.006125</td>\n",
       "      <td>1.120102</td>\n",
       "      <td>1.287057</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>1.291705</td>\n",
       "      <td>1.006085</td>\n",
       "      <td>1.133866</td>\n",
       "      <td>1.997868</td>\n",
       "      <td>...</td>\n",
       "      <td>1.062240</td>\n",
       "      <td>1.301587</td>\n",
       "      <td>0.354315</td>\n",
       "      <td>0.886438</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.836663</td>\n",
       "      <td>2.356522</td>\n",
       "      <td>1.211736</td>\n",
       "      <td>0.346785</td>\n",
       "      <td>1.116860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-09</th>\n",
       "      <td>1.403873</td>\n",
       "      <td>2.113962</td>\n",
       "      <td>1.005121</td>\n",
       "      <td>1.109142</td>\n",
       "      <td>1.253619</td>\n",
       "      <td>0.070737</td>\n",
       "      <td>1.283938</td>\n",
       "      <td>1.006085</td>\n",
       "      <td>1.151779</td>\n",
       "      <td>2.017058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040166</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.332995</td>\n",
       "      <td>0.878348</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>0.881316</td>\n",
       "      <td>2.339339</td>\n",
       "      <td>1.199516</td>\n",
       "      <td>0.350205</td>\n",
       "      <td>1.106767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-10</th>\n",
       "      <td>1.378062</td>\n",
       "      <td>2.019984</td>\n",
       "      <td>1.006125</td>\n",
       "      <td>1.098182</td>\n",
       "      <td>1.216088</td>\n",
       "      <td>0.067014</td>\n",
       "      <td>1.260830</td>\n",
       "      <td>1.006085</td>\n",
       "      <td>1.120673</td>\n",
       "      <td>1.972281</td>\n",
       "      <td>...</td>\n",
       "      <td>1.037474</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.338071</td>\n",
       "      <td>0.862168</td>\n",
       "      <td>0.819835</td>\n",
       "      <td>0.733255</td>\n",
       "      <td>2.353577</td>\n",
       "      <td>1.230064</td>\n",
       "      <td>0.344049</td>\n",
       "      <td>1.097791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-13</th>\n",
       "      <td>1.324021</td>\n",
       "      <td>1.941399</td>\n",
       "      <td>1.004117</td>\n",
       "      <td>1.097174</td>\n",
       "      <td>1.162290</td>\n",
       "      <td>0.057930</td>\n",
       "      <td>1.237140</td>\n",
       "      <td>1.014199</td>\n",
       "      <td>1.035706</td>\n",
       "      <td>1.846482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998710</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.315736</td>\n",
       "      <td>0.835586</td>\n",
       "      <td>0.737190</td>\n",
       "      <td>0.730905</td>\n",
       "      <td>2.291227</td>\n",
       "      <td>1.124165</td>\n",
       "      <td>0.329685</td>\n",
       "      <td>1.077440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-07</th>\n",
       "      <td>1.259090</td>\n",
       "      <td>1.821766</td>\n",
       "      <td>0.202832</td>\n",
       "      <td>1.134360</td>\n",
       "      <td>1.242030</td>\n",
       "      <td>0.073716</td>\n",
       "      <td>1.268203</td>\n",
       "      <td>1.022312</td>\n",
       "      <td>1.027596</td>\n",
       "      <td>1.611940</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018092</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.265990</td>\n",
       "      <td>0.841365</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>1.948549</td>\n",
       "      <td>1.136384</td>\n",
       "      <td>0.343365</td>\n",
       "      <td>1.064245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-08</th>\n",
       "      <td>1.262720</td>\n",
       "      <td>1.890359</td>\n",
       "      <td>0.171704</td>\n",
       "      <td>1.133779</td>\n",
       "      <td>1.254682</td>\n",
       "      <td>0.077439</td>\n",
       "      <td>1.286874</td>\n",
       "      <td>1.022312</td>\n",
       "      <td>1.047930</td>\n",
       "      <td>1.656716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.019169</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.265990</td>\n",
       "      <td>0.835586</td>\n",
       "      <td>0.659504</td>\n",
       "      <td>0.707403</td>\n",
       "      <td>1.895037</td>\n",
       "      <td>1.124165</td>\n",
       "      <td>0.348153</td>\n",
       "      <td>1.064783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-09</th>\n",
       "      <td>1.298613</td>\n",
       "      <td>1.930597</td>\n",
       "      <td>0.156642</td>\n",
       "      <td>1.139096</td>\n",
       "      <td>1.276690</td>\n",
       "      <td>0.078183</td>\n",
       "      <td>1.297093</td>\n",
       "      <td>1.022312</td>\n",
       "      <td>1.062455</td>\n",
       "      <td>1.692964</td>\n",
       "      <td>...</td>\n",
       "      <td>1.027783</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.268020</td>\n",
       "      <td>0.836742</td>\n",
       "      <td>0.674380</td>\n",
       "      <td>0.735605</td>\n",
       "      <td>1.894055</td>\n",
       "      <td>1.138421</td>\n",
       "      <td>0.374829</td>\n",
       "      <td>1.073552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12</th>\n",
       "      <td>1.311922</td>\n",
       "      <td>1.944910</td>\n",
       "      <td>0.151622</td>\n",
       "      <td>1.133466</td>\n",
       "      <td>1.287960</td>\n",
       "      <td>0.083395</td>\n",
       "      <td>1.295914</td>\n",
       "      <td>1.022312</td>\n",
       "      <td>1.064633</td>\n",
       "      <td>1.752665</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036936</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.275127</td>\n",
       "      <td>0.841365</td>\n",
       "      <td>0.692562</td>\n",
       "      <td>0.781434</td>\n",
       "      <td>2.017772</td>\n",
       "      <td>1.142494</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>1.074628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-13</th>\n",
       "      <td>1.270382</td>\n",
       "      <td>1.906562</td>\n",
       "      <td>0.149613</td>\n",
       "      <td>1.125871</td>\n",
       "      <td>1.237671</td>\n",
       "      <td>0.079672</td>\n",
       "      <td>1.272134</td>\n",
       "      <td>1.021298</td>\n",
       "      <td>1.025781</td>\n",
       "      <td>1.665245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998710</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.256853</td>\n",
       "      <td>0.833275</td>\n",
       "      <td>0.669421</td>\n",
       "      <td>0.727380</td>\n",
       "      <td>1.946095</td>\n",
       "      <td>1.109909</td>\n",
       "      <td>0.370041</td>\n",
       "      <td>1.055890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 10569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                GRES      ONTO      NOGN     COWNL      SIZE      HEPS  \\\n",
       "2022-06-07  1.452672  2.259519  1.006125  1.133254  1.303582  0.074460   \n",
       "2022-06-08  1.437347  2.178774  1.006125  1.120102  1.287057  0.074460   \n",
       "2022-06-09  1.403873  2.113962  1.005121  1.109142  1.253619  0.070737   \n",
       "2022-06-10  1.378062  2.019984  1.006125  1.098182  1.216088  0.067014   \n",
       "2022-06-13  1.324021  1.941399  1.004117  1.097174  1.162290  0.057930   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-09-07  1.259090  1.821766  0.202832  1.134360  1.242030  0.073716   \n",
       "2022-09-08  1.262720  1.890359  0.171704  1.133779  1.254682  0.077439   \n",
       "2022-09-09  1.298613  1.930597  0.156642  1.139096  1.276690  0.078183   \n",
       "2022-09-12  1.311922  1.944910  0.151622  1.133466  1.287960  0.083395   \n",
       "2022-09-13  1.270382  1.906562  0.149613  1.125871  1.237671  0.079672   \n",
       "\n",
       "                 DXJ      FRBN       REX        PR  ...      GOOD       IGC  \\\n",
       "2022-06-07  1.295589  1.006085  1.149843  2.040512  ...  1.081622  0.666667   \n",
       "2022-06-08  1.291705  1.006085  1.133866  1.997868  ...  1.062240  1.301587   \n",
       "2022-06-09  1.283938  1.006085  1.151779  2.017058  ...  1.040166  0.984127   \n",
       "2022-06-10  1.260830  1.006085  1.120673  1.972281  ...  1.037474  0.825397   \n",
       "2022-06-13  1.237140  1.014199  1.035706  1.846482  ...  0.998710  0.761905   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2022-09-07  1.268203  1.022312  1.027596  1.611940  ...  1.018092  0.873016   \n",
       "2022-09-08  1.286874  1.022312  1.047930  1.656716  ...  1.019169  0.888889   \n",
       "2022-09-09  1.297093  1.022312  1.062455  1.692964  ...  1.027783  0.873016   \n",
       "2022-09-12  1.295914  1.022312  1.064633  1.752665  ...  1.036936  0.920635   \n",
       "2022-09-13  1.272134  1.021298  1.025781  1.665245  ...  0.998710  0.904762   \n",
       "\n",
       "                OPFI       JPS      CNDT      APYX      KRBN       GCV  \\\n",
       "2022-06-07  0.322843  0.882971  0.912397  0.853114  2.352104  1.213772   \n",
       "2022-06-08  0.354315  0.886438  0.890909  0.836663  2.356522  1.211736   \n",
       "2022-06-09  0.332995  0.878348  0.867769  0.881316  2.339339  1.199516   \n",
       "2022-06-10  0.338071  0.862168  0.819835  0.733255  2.353577  1.230064   \n",
       "2022-06-13  0.315736  0.835586  0.737190  0.730905  2.291227  1.124165   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-09-07  0.265990  0.841365  0.669421  0.702703  1.948549  1.136384   \n",
       "2022-09-08  0.265990  0.835586  0.659504  0.707403  1.895037  1.124165   \n",
       "2022-09-09  0.268020  0.836742  0.674380  0.735605  1.894055  1.138421   \n",
       "2022-09-12  0.275127  0.841365  0.692562  0.781434  2.017772  1.142494   \n",
       "2022-09-13  0.256853  0.833275  0.669421  0.727380  1.946095  1.109909   \n",
       "\n",
       "                FARM      WBIG  \n",
       "2022-06-07  0.350889  1.125753  \n",
       "2022-06-08  0.346785  1.116860  \n",
       "2022-06-09  0.350205  1.106767  \n",
       "2022-06-10  0.344049  1.097791  \n",
       "2022-06-13  0.329685  1.077440  \n",
       "...              ...       ...  \n",
       "2022-09-07  0.343365  1.064245  \n",
       "2022-09-08  0.348153  1.064783  \n",
       "2022-09-09  0.374829  1.073552  \n",
       "2022-09-12  0.372093  1.074628  \n",
       "2022-09-13  0.370041  1.055890  \n",
       "\n",
       "[68 rows x 10569 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=data_df.iloc[:-68]\n",
    "test_df=data_df.iloc[-68:]\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a7d369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1644 stocks were droped\n"
     ]
    }
   ],
   "source": [
    "ticker_list=data_df.columns\n",
    "print(f\"{len(symbols)-len(ticker_list)} stocks were droped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7efabaa",
   "metadata": {},
   "source": [
    "# preprocessing \n",
    "\n",
    "\n",
    "1. ticker: this is also used to select which stocks come to the model\n",
    "\n",
    "2. price: we take the ratio: (importantly we did drop some potentialy important indicators)\n",
    "\n",
    "3. date: the day and month are floating point numbers. while year and day of the week are embedings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "673271cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "look=layers.StringLookup(vocabulary=ticker_list,num_oov_indices=0)\n",
    "inv_look=layers.StringLookup(vocabulary=ticker_list,invert=True,num_oov_indices=0)\n",
    "tick_embed=layers.Embedding(len(ticker_list)+1,512,embeddings_constraint= tf.keras.constraints.unit_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "47802c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(17,), dtype=string, numpy=\n",
       "array([b'GRES', b'ONTO', b'NOGN', b'COWNL', b'SIZE', b'HEPS', b'DXJ',\n",
       "       b'FRBN', b'REX', b'PR', b'MDWD', b'LEVI', b'HMC', b'AIKI', b'SPUC',\n",
       "       b'FANG', b'SAVN'], dtype=object)>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=tf.data.Dataset.from_tensor_slices(ticker_list)\n",
    "exmple=dataset.batch(17).take(1).get_single_element()\n",
    "exmple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a701202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df[exmple.numpy().astype('U13')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "251a654f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[range(0, 0), range(0, 1), range(0, 2), range(0, 3), range(0, 4)],\n",
       " [range(0, 0), range(0, 1), range(0, 2), range(0, 3), range(0, 4)]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def thread_distribute(l,f,n=1):\n",
    "    if(n==0):\n",
    "        return f(l)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor: \n",
    "        l=[executor.submit(lambda x: thread_distribute(x,f,n-1),x) for x in l]\n",
    "        l= [x.result() for x in l]\n",
    "    return l\n",
    "\n",
    "thread_distribute([range(5) for i in range(2)],lambda x: range(x),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9d5f1e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRES</th>\n",
       "      <th>OST</th>\n",
       "      <th>LYTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-04-27</th>\n",
       "      <td>1.369593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-28</th>\n",
       "      <td>1.392581</td>\n",
       "      <td>0.115986</td>\n",
       "      <td>1.195014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-29</th>\n",
       "      <td>1.368383</td>\n",
       "      <td>0.093293</td>\n",
       "      <td>1.229206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-02</th>\n",
       "      <td>1.365963</td>\n",
       "      <td>0.076147</td>\n",
       "      <td>1.309557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-03</th>\n",
       "      <td>1.386128</td>\n",
       "      <td>0.119768</td>\n",
       "      <td>1.282203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-04</th>\n",
       "      <td>1.412746</td>\n",
       "      <td>0.095814</td>\n",
       "      <td>1.282203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-05</th>\n",
       "      <td>1.390968</td>\n",
       "      <td>0.083207</td>\n",
       "      <td>1.275365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-06</th>\n",
       "      <td>1.388144</td>\n",
       "      <td>0.076399</td>\n",
       "      <td>1.201852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-09</th>\n",
       "      <td>1.325230</td>\n",
       "      <td>0.066062</td>\n",
       "      <td>1.225786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-10</th>\n",
       "      <td>1.323214</td>\n",
       "      <td>0.063540</td>\n",
       "      <td>1.205271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-11</th>\n",
       "      <td>1.331280</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>1.189885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-12</th>\n",
       "      <td>1.320391</td>\n",
       "      <td>0.054463</td>\n",
       "      <td>1.198433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-13</th>\n",
       "      <td>1.349428</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>1.184756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-16</th>\n",
       "      <td>1.363544</td>\n",
       "      <td>0.059002</td>\n",
       "      <td>1.186466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-17</th>\n",
       "      <td>1.387741</td>\n",
       "      <td>0.058497</td>\n",
       "      <td>1.181337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-18</th>\n",
       "      <td>1.341362</td>\n",
       "      <td>0.065280</td>\n",
       "      <td>1.186466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-19</th>\n",
       "      <td>1.353058</td>\n",
       "      <td>0.058875</td>\n",
       "      <td>1.147145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-20</th>\n",
       "      <td>1.354671</td>\n",
       "      <td>0.054967</td>\n",
       "      <td>1.153983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-23</th>\n",
       "      <td>1.384515</td>\n",
       "      <td>0.052194</td>\n",
       "      <td>1.145435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-24</th>\n",
       "      <td>1.387338</td>\n",
       "      <td>0.046646</td>\n",
       "      <td>1.080470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-25</th>\n",
       "      <td>1.403470</td>\n",
       "      <td>0.048159</td>\n",
       "      <td>1.109533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-26</th>\n",
       "      <td>1.407099</td>\n",
       "      <td>0.049672</td>\n",
       "      <td>1.124920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-27</th>\n",
       "      <td>1.423231</td>\n",
       "      <td>0.053454</td>\n",
       "      <td>1.157402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31</th>\n",
       "      <td>1.416375</td>\n",
       "      <td>0.051437</td>\n",
       "      <td>1.142016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>1.414359</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>1.188175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-02</th>\n",
       "      <td>1.434927</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>1.186466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03</th>\n",
       "      <td>1.429281</td>\n",
       "      <td>0.050429</td>\n",
       "      <td>1.171079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>1.432507</td>\n",
       "      <td>0.049420</td>\n",
       "      <td>1.164241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GRES       OST      LYTS\n",
       "2022-04-27  1.369593  1.000000  1.000119\n",
       "2022-04-28  1.392581  0.115986  1.195014\n",
       "2022-04-29  1.368383  0.093293  1.229206\n",
       "2022-05-02  1.365963  0.076147  1.309557\n",
       "2022-05-03  1.386128  0.119768  1.282203\n",
       "2022-05-04  1.412746  0.095814  1.282203\n",
       "2022-05-05  1.390968  0.083207  1.275365\n",
       "2022-05-06  1.388144  0.076399  1.201852\n",
       "2022-05-09  1.325230  0.066062  1.225786\n",
       "2022-05-10  1.323214  0.063540  1.205271\n",
       "2022-05-11  1.331280  0.055976  1.189885\n",
       "2022-05-12  1.320391  0.054463  1.198433\n",
       "2022-05-13  1.349428  0.062027  1.184756\n",
       "2022-05-16  1.363544  0.059002  1.186466\n",
       "2022-05-17  1.387741  0.058497  1.181337\n",
       "2022-05-18  1.341362  0.065280  1.186466\n",
       "2022-05-19  1.353058  0.058875  1.147145\n",
       "2022-05-20  1.354671  0.054967  1.153983\n",
       "2022-05-23  1.384515  0.052194  1.145435\n",
       "2022-05-24  1.387338  0.046646  1.080470\n",
       "2022-05-25  1.403470  0.048159  1.109533\n",
       "2022-05-26  1.407099  0.049672  1.124920\n",
       "2022-05-27  1.423231  0.053454  1.157402\n",
       "2022-05-31  1.416375  0.051437  1.142016\n",
       "2022-06-01  1.414359  0.051942  1.188175\n",
       "2022-06-02  1.434927  0.051689  1.186466\n",
       "2022-06-03  1.429281  0.050429  1.171079\n",
       "2022-06-06  1.432507  0.049420  1.164241"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_stocks(x,data_df=train_df):\n",
    "    a=tick_embed(look(ticker_list))\n",
    "    b=tick_embed(look(x))\n",
    "\n",
    "    b=b@tf.transpose(a)\n",
    "    b=tf.nn.top_k(b,3)\n",
    "\n",
    "\n",
    "    tickers=inv_look([list(x) for x in b[1].numpy()]).numpy().astype('U13')\n",
    "    \n",
    "    return [[data_df[x] for x in sub] for sub in tickers]\n",
    "    #return thread_distribute(tickers,lambda x: data_df[x],2)\n",
    "\n",
    "a=choose_stocks(exmple)\n",
    "\n",
    "pd.DataFrame(a[0]).transpose().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d8a28548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-01-02', '2020-01-03', '2020-01-06', '2020-01-07',\n",
       "               '2020-01-08', '2020-01-09', '2020-01-10', '2020-01-13',\n",
       "               '2020-01-14', '2020-01-15',\n",
       "               ...\n",
       "               '2022-05-23', '2022-05-24', '2022-05-25', '2022-05-26',\n",
       "               '2022-05-27', '2022-05-31', '2022-06-01', '2022-06-02',\n",
       "               '2022-06-03', '2022-06-06'],\n",
       "              dtype='datetime64[ns]', length=612, freq=None)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9fd4c11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0.06666666666666667, 0.08333333333333333, 0)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time_encode(a):\n",
    "    #a=a.index\n",
    "    return (a.day_of_week,a.day/30,a.month/12,(a.year-2020))\n",
    "    \n",
    "time_encode(data_df.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "23d387f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(17, 3, 612), dtype=int32, numpy=\n",
       " array([[[3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0]],\n",
       " \n",
       "        [[3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0]],\n",
       " \n",
       "        [[3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0]],\n",
       " \n",
       "        [[3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0]],\n",
       " \n",
       "        [[3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0],\n",
       "         [3, 4, 0, ..., 3, 4, 0]]])>,\n",
       " <tf.Tensor: shape=(17, 3, 612), dtype=float64, numpy=\n",
       " array([[[0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ]],\n",
       " \n",
       "        [[0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ]],\n",
       " \n",
       "        [[0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ]],\n",
       " \n",
       "        [[0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ]],\n",
       " \n",
       "        [[0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ],\n",
       "         [0.06666667, 0.1       , 0.2       , ..., 0.06666667,\n",
       "          0.1       , 0.2       ]]])>,\n",
       " <tf.Tensor: shape=(17, 3, 612), dtype=float64, numpy=\n",
       " array([[[0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ]],\n",
       " \n",
       "        [[0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ]],\n",
       " \n",
       "        [[0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ]],\n",
       " \n",
       "        [[0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ]],\n",
       " \n",
       "        [[0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ],\n",
       "         [0.08333333, 0.08333333, 0.08333333, ..., 0.5       ,\n",
       "          0.5       , 0.5       ]]])>,\n",
       " <tf.Tensor: shape=(17, 3, 612), dtype=int32, numpy=\n",
       " array([[[0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2]],\n",
       " \n",
       "        [[0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2]],\n",
       " \n",
       "        [[0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2]],\n",
       " \n",
       "        [[0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2]],\n",
       " \n",
       "        [[0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2],\n",
       "         [0, 0, 0, ..., 2, 2, 2]]])>]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time_tensors(a):\n",
    "    x=thread_distribute(a,lambda x:time_encode(x.index),2)\n",
    "    x=[thread_distribute(x,lambda x: x[i],2) for i in range(4)]\n",
    "    return [tf.constant(x) for x in x]\n",
    "\n",
    "time_tensors(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa725f",
   "metadata": {},
   "source": [
    "# time handeling \n",
    "\n",
    "problems with the data in curent time format:\n",
    "\n",
    "1. there are null entries especialy from before a company was made \n",
    "2. the time diffrence between entries isnt clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "16159624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False, ...,  True,  True,  True],\n",
       "       [ True,  True, False, ...,  True,  True,  True],\n",
       "       [ True,  True, False, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=np.logical_not(data_df.isna().to_numpy())\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a993565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRES</th>\n",
       "      <th>OST</th>\n",
       "      <th>LYTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-04-27</th>\n",
       "      <td>1.369593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-28</th>\n",
       "      <td>1.392581</td>\n",
       "      <td>0.115986</td>\n",
       "      <td>1.195014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-29</th>\n",
       "      <td>1.368383</td>\n",
       "      <td>0.093293</td>\n",
       "      <td>1.229206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-02</th>\n",
       "      <td>1.365963</td>\n",
       "      <td>0.076147</td>\n",
       "      <td>1.309557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-03</th>\n",
       "      <td>1.386128</td>\n",
       "      <td>0.119768</td>\n",
       "      <td>1.282203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-04</th>\n",
       "      <td>1.412746</td>\n",
       "      <td>0.095814</td>\n",
       "      <td>1.282203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-05</th>\n",
       "      <td>1.390968</td>\n",
       "      <td>0.083207</td>\n",
       "      <td>1.275365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-06</th>\n",
       "      <td>1.388144</td>\n",
       "      <td>0.076399</td>\n",
       "      <td>1.201852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-09</th>\n",
       "      <td>1.325230</td>\n",
       "      <td>0.066062</td>\n",
       "      <td>1.225786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-10</th>\n",
       "      <td>1.323214</td>\n",
       "      <td>0.063540</td>\n",
       "      <td>1.205271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-11</th>\n",
       "      <td>1.331280</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>1.189885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-12</th>\n",
       "      <td>1.320391</td>\n",
       "      <td>0.054463</td>\n",
       "      <td>1.198433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-13</th>\n",
       "      <td>1.349428</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>1.184756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-16</th>\n",
       "      <td>1.363544</td>\n",
       "      <td>0.059002</td>\n",
       "      <td>1.186466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-17</th>\n",
       "      <td>1.387741</td>\n",
       "      <td>0.058497</td>\n",
       "      <td>1.181337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-18</th>\n",
       "      <td>1.341362</td>\n",
       "      <td>0.065280</td>\n",
       "      <td>1.186466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-19</th>\n",
       "      <td>1.353058</td>\n",
       "      <td>0.058875</td>\n",
       "      <td>1.147145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-20</th>\n",
       "      <td>1.354671</td>\n",
       "      <td>0.054967</td>\n",
       "      <td>1.153983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-23</th>\n",
       "      <td>1.384515</td>\n",
       "      <td>0.052194</td>\n",
       "      <td>1.145435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-24</th>\n",
       "      <td>1.387338</td>\n",
       "      <td>0.046646</td>\n",
       "      <td>1.080470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-25</th>\n",
       "      <td>1.403470</td>\n",
       "      <td>0.048159</td>\n",
       "      <td>1.109533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-26</th>\n",
       "      <td>1.407099</td>\n",
       "      <td>0.049672</td>\n",
       "      <td>1.124920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-27</th>\n",
       "      <td>1.423231</td>\n",
       "      <td>0.053454</td>\n",
       "      <td>1.157402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31</th>\n",
       "      <td>1.416375</td>\n",
       "      <td>0.051437</td>\n",
       "      <td>1.142016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>1.414359</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>1.188175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-02</th>\n",
       "      <td>1.434927</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>1.186466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03</th>\n",
       "      <td>1.429281</td>\n",
       "      <td>0.050429</td>\n",
       "      <td>1.171079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>1.432507</td>\n",
       "      <td>0.049420</td>\n",
       "      <td>1.164241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GRES       OST      LYTS\n",
       "2022-04-27  1.369593  1.000000  1.000119\n",
       "2022-04-28  1.392581  0.115986  1.195014\n",
       "2022-04-29  1.368383  0.093293  1.229206\n",
       "2022-05-02  1.365963  0.076147  1.309557\n",
       "2022-05-03  1.386128  0.119768  1.282203\n",
       "2022-05-04  1.412746  0.095814  1.282203\n",
       "2022-05-05  1.390968  0.083207  1.275365\n",
       "2022-05-06  1.388144  0.076399  1.201852\n",
       "2022-05-09  1.325230  0.066062  1.225786\n",
       "2022-05-10  1.323214  0.063540  1.205271\n",
       "2022-05-11  1.331280  0.055976  1.189885\n",
       "2022-05-12  1.320391  0.054463  1.198433\n",
       "2022-05-13  1.349428  0.062027  1.184756\n",
       "2022-05-16  1.363544  0.059002  1.186466\n",
       "2022-05-17  1.387741  0.058497  1.181337\n",
       "2022-05-18  1.341362  0.065280  1.186466\n",
       "2022-05-19  1.353058  0.058875  1.147145\n",
       "2022-05-20  1.354671  0.054967  1.153983\n",
       "2022-05-23  1.384515  0.052194  1.145435\n",
       "2022-05-24  1.387338  0.046646  1.080470\n",
       "2022-05-25  1.403470  0.048159  1.109533\n",
       "2022-05-26  1.407099  0.049672  1.124920\n",
       "2022-05-27  1.423231  0.053454  1.157402\n",
       "2022-05-31  1.416375  0.051437  1.142016\n",
       "2022-06-01  1.414359  0.051942  1.188175\n",
       "2022-06-02  1.434927  0.051689  1.186466\n",
       "2022-06-03  1.429281  0.050429  1.171079\n",
       "2022-06-06  1.432507  0.049420  1.164241"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_stocks_df(x,data_df=train_df):\n",
    "    a=tick_embed(look(ticker_list))\n",
    "    b=tick_embed(look(x))\n",
    "\n",
    "    b=b@tf.transpose(a)\n",
    "    b=tf.nn.top_k(b,3)\n",
    "\n",
    "\n",
    "    tickers=inv_look([list(x) for x in b[1].numpy()]).numpy().astype('U13')\n",
    "    \n",
    "    \n",
    "    return [pd.DataFrame([data_df[x] for x in sub]).transpose().dropna() for sub in tickers]\n",
    "\n",
    "v=choose_stocks_df(exmple)\n",
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f75c44e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(TensorShape([17, 612]), tf.int32),\n",
       " (TensorShape([17, 612]), tf.float32),\n",
       " (TensorShape([17, 612]), tf.float32),\n",
       " (TensorShape([17, 612]), tf.int32),\n",
       " (TensorShape([17, 612]), tf.float64),\n",
       " (TensorShape([17, 612]), tf.float64),\n",
       " (TensorShape([17, 612]), tf.float64),\n",
       " (TensorShape([17, 3]), tf.string)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to add the tickers strings/ints\n",
    "def get_xs(v):\n",
    "    t=[x.index for x in v]\n",
    "    t=thread_distribute(t,time_encode,2)\n",
    "    #t=[[x[i] for x in t] for i in range(4)]\n",
    "    t=[thread_distribute(t,lambda x: x[i],2) for i in range(4)]\n",
    "    #thread_distribute(t,len,2)\n",
    "    t=[tf.ragged.constant(x) for x in t]\n",
    "    \n",
    "    x=thread_distribute(v,lambda x: x.to_numpy(),1)\n",
    "    vals=tf.concat(x,axis=0)\n",
    "    lengths=[len(s) for s in x]\n",
    "    x=tf.RaggedTensor.from_row_lengths(vals,lengths)\n",
    "    #[x.shape for x in x]\n",
    "    \n",
    "    \"padding\"\n",
    "    x=(x+1).to_tensor()-1\n",
    "    t=[(x+1).to_tensor()-1 for x in t] \n",
    "    \n",
    "    x=tf.transpose(x,[2,0,1])\n",
    "    t.extend(x) \n",
    "    \n",
    "    tickers=tf.constant([x.transpose().index for x in v])\n",
    "    t.append(tickers)\n",
    "    return t\n",
    "\n",
    "xs=get_xs(v)\n",
    "[(x.shape,x.dtype) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "755a743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[len(x) for x in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e8e380cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRES</th>\n",
       "      <th>OST</th>\n",
       "      <th>LYTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-04-27</th>\n",
       "      <td>1.369593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-28</th>\n",
       "      <td>1.392581</td>\n",
       "      <td>0.115986</td>\n",
       "      <td>1.195014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-29</th>\n",
       "      <td>1.368383</td>\n",
       "      <td>0.093293</td>\n",
       "      <td>1.229206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-02</th>\n",
       "      <td>1.365963</td>\n",
       "      <td>0.076147</td>\n",
       "      <td>1.309557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-03</th>\n",
       "      <td>1.386128</td>\n",
       "      <td>0.119768</td>\n",
       "      <td>1.282203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-04</th>\n",
       "      <td>1.412746</td>\n",
       "      <td>0.095814</td>\n",
       "      <td>1.282203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-05</th>\n",
       "      <td>1.390968</td>\n",
       "      <td>0.083207</td>\n",
       "      <td>1.275365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-06</th>\n",
       "      <td>1.388144</td>\n",
       "      <td>0.076399</td>\n",
       "      <td>1.201852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-09</th>\n",
       "      <td>1.325230</td>\n",
       "      <td>0.066062</td>\n",
       "      <td>1.225786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-10</th>\n",
       "      <td>1.323214</td>\n",
       "      <td>0.063540</td>\n",
       "      <td>1.205271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-11</th>\n",
       "      <td>1.331280</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>1.189885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-12</th>\n",
       "      <td>1.320391</td>\n",
       "      <td>0.054463</td>\n",
       "      <td>1.198433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-13</th>\n",
       "      <td>1.349428</td>\n",
       "      <td>0.062027</td>\n",
       "      <td>1.184756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-16</th>\n",
       "      <td>1.363544</td>\n",
       "      <td>0.059002</td>\n",
       "      <td>1.186466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-17</th>\n",
       "      <td>1.387741</td>\n",
       "      <td>0.058497</td>\n",
       "      <td>1.181337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-18</th>\n",
       "      <td>1.341362</td>\n",
       "      <td>0.065280</td>\n",
       "      <td>1.186466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-19</th>\n",
       "      <td>1.353058</td>\n",
       "      <td>0.058875</td>\n",
       "      <td>1.147145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-20</th>\n",
       "      <td>1.354671</td>\n",
       "      <td>0.054967</td>\n",
       "      <td>1.153983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-23</th>\n",
       "      <td>1.384515</td>\n",
       "      <td>0.052194</td>\n",
       "      <td>1.145435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-24</th>\n",
       "      <td>1.387338</td>\n",
       "      <td>0.046646</td>\n",
       "      <td>1.080470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-25</th>\n",
       "      <td>1.403470</td>\n",
       "      <td>0.048159</td>\n",
       "      <td>1.109533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-26</th>\n",
       "      <td>1.407099</td>\n",
       "      <td>0.049672</td>\n",
       "      <td>1.124920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-27</th>\n",
       "      <td>1.423231</td>\n",
       "      <td>0.053454</td>\n",
       "      <td>1.157402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-31</th>\n",
       "      <td>1.416375</td>\n",
       "      <td>0.051437</td>\n",
       "      <td>1.142016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>1.414359</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>1.188175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-02</th>\n",
       "      <td>1.434927</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>1.186466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-03</th>\n",
       "      <td>1.429281</td>\n",
       "      <td>0.050429</td>\n",
       "      <td>1.171079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>1.432507</td>\n",
       "      <td>0.049420</td>\n",
       "      <td>1.164241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                GRES       OST      LYTS\n",
       "2022-04-27  1.369593  1.000000  1.000119\n",
       "2022-04-28  1.392581  0.115986  1.195014\n",
       "2022-04-29  1.368383  0.093293  1.229206\n",
       "2022-05-02  1.365963  0.076147  1.309557\n",
       "2022-05-03  1.386128  0.119768  1.282203\n",
       "2022-05-04  1.412746  0.095814  1.282203\n",
       "2022-05-05  1.390968  0.083207  1.275365\n",
       "2022-05-06  1.388144  0.076399  1.201852\n",
       "2022-05-09  1.325230  0.066062  1.225786\n",
       "2022-05-10  1.323214  0.063540  1.205271\n",
       "2022-05-11  1.331280  0.055976  1.189885\n",
       "2022-05-12  1.320391  0.054463  1.198433\n",
       "2022-05-13  1.349428  0.062027  1.184756\n",
       "2022-05-16  1.363544  0.059002  1.186466\n",
       "2022-05-17  1.387741  0.058497  1.181337\n",
       "2022-05-18  1.341362  0.065280  1.186466\n",
       "2022-05-19  1.353058  0.058875  1.147145\n",
       "2022-05-20  1.354671  0.054967  1.153983\n",
       "2022-05-23  1.384515  0.052194  1.145435\n",
       "2022-05-24  1.387338  0.046646  1.080470\n",
       "2022-05-25  1.403470  0.048159  1.109533\n",
       "2022-05-26  1.407099  0.049672  1.124920\n",
       "2022-05-27  1.423231  0.053454  1.157402\n",
       "2022-05-31  1.416375  0.051437  1.142016\n",
       "2022-06-01  1.414359  0.051942  1.188175\n",
       "2022-06-02  1.434927  0.051689  1.186466\n",
       "2022-06-03  1.429281  0.050429  1.171079\n",
       "2022-06-06  1.432507  0.049420  1.164241"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_seq(x,LEN=100):\n",
    "    l=len(x)\n",
    "    if(l<LEN):\n",
    "        return x \n",
    "    \n",
    "    start=np.random.randint(l-LEN)\n",
    "    return x.iloc[start:start+LEN]\n",
    "sample_seq(v[-1]) \n",
    "sample_seq(v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "af077c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "base=[sample_seq(x) for x in v]\n",
    "y=[s.iloc[1:,0] for s in base]\n",
    "x=[s[:-1] for s in base]\n",
    "x=get_xs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "dd0f22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9607014656066895\n"
     ]
    }
   ],
   "source": [
    "def get_numbers(tickers):\n",
    "    v=choose_stocks_df(tickers)\n",
    "    base=[sample_seq(x) for x in v]\n",
    "    \n",
    "    y=[s.iloc[1:,0].to_numpy() for s in base]\n",
    "    y=tf.ragged.constant(y)\n",
    "    y=(y+1).to_tensor()-1\n",
    "    \n",
    "    x=[s[:-1] for s in base]\n",
    "    x=get_xs(x)\n",
    "    #x.append(tickers)\n",
    "    x.append(y)\n",
    "    return x\n",
    "t=time.time()\n",
    "n=get_numbers(exmple)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "73d7c216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorSpec(shape=(None, 99), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(None, 99), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 99), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 99), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(None, 99), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 99), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 99), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 3), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(None, 99), dtype=tf.float64, name=None)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specs=[tf.TensorSpec.from_tensor(x) for x in n]\n",
    "s=[list(x.shape) for x in specs]\n",
    "for x in s:\n",
    "    x[0]=None\n",
    "specs=[tf.TensorSpec(s[i],specs[i].dtype) for i in range(len(specs))]\n",
    "specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3332fee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2, 3), 4)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gather_x(*args):\n",
    "    return args[:-1]\n",
    "\n",
    "def gather_y(*args):\n",
    "    return args[-1]\n",
    "\n",
    "gather_x(1,2,3,4),gather_y(1,2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec2359",
   "metadata": {},
   "source": [
    "# train dataset \n",
    "because of the need to use the dataset as a refrence type in the pandas df we are forcing tensorflow to run the whole thing eagerly. \n",
    "\n",
    "it shouldnt be a major issue since most the heavy lifting is done in pandas which is optimized c code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "aa0fa0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to do training and test set (test set should look diffrent)\n",
    "train_dataset=dataset.shuffle(1000).batch(32).map(lambda y: tf.py_function(get_numbers,inp=[y],Tout=specs))\n",
    "\n",
    "dataset_x=train_dataset.map(gather_x)\n",
    "dataset_y=train_dataset.map(gather_y)\n",
    "train_dataset=tf.data.Dataset.zip((dataset_x,dataset_y))\n",
    "\n",
    "x=train_dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "04dd2a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorSpec(shape=(32, 99), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(32, 99), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(32, 99), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(32, 99), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(32, 99), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(32, 99), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(32, 99), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(32, 3), dtype=tf.string, name=None)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf.TensorSpec.from_tensor(x) for x in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "799cb364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': (<tf.Tensor: shape=(32, 99), dtype=int32, numpy=\n",
       "  array([[ 2,  3,  4, ..., -1, -1, -1],\n",
       "         [ 3,  4,  0, ..., -1, -1, -1],\n",
       "         [ 4,  0,  1, ..., -1, -1, -1],\n",
       "         ...,\n",
       "         [ 1,  2,  4, ...,  1,  2,  3],\n",
       "         [ 1,  2,  3, ...,  4,  0,  1],\n",
       "         [ 4,  0,  1, ...,  4,  0,  1]])>,\n",
       "  <tf.Tensor: shape=(32, 99), dtype=float32, numpy=\n",
       "  array([[ 0.29999995,  0.33333337,  0.36666667, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 0.33333337,  0.36666667,  0.4666667 , ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 0.36666667,  0.4666667 ,  0.5       , ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         ...,\n",
       "         [ 0.76666665,  0.79999995,  0.8666667 , ...,  0.39999998,\n",
       "           0.4333334 ,  0.4666667 ],\n",
       "         [ 0.10000002,  0.13333333,  0.16666663, ...,  0.5666666 ,\n",
       "           0.66666675,  0.70000005],\n",
       "         [ 0.33333337,  0.4333334 ,  0.4666667 , ...,  0.9666667 ,\n",
       "           0.06666672,  0.10000002]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(32, 99), dtype=float32, numpy=\n",
       "  array([[ 0.16666663,  0.16666663,  0.16666663, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 0.16666663,  0.16666663,  0.16666663, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 0.16666663,  0.16666663,  0.16666663, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         ...,\n",
       "         [ 0.91666675,  0.91666675,  0.91666675, ...,  0.33333337,\n",
       "           0.33333337,  0.33333337],\n",
       "         [ 0.66666675,  0.66666675,  0.66666675, ...,  1.        ,\n",
       "           1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        , ...,  0.33333337,\n",
       "           0.41666663,  0.41666663]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(32, 99), dtype=int32, numpy=\n",
       "  array([[ 2,  2,  2, ..., -1, -1, -1],\n",
       "         [ 2,  2,  2, ..., -1, -1, -1],\n",
       "         [ 2,  2,  2, ..., -1, -1, -1],\n",
       "         ...,\n",
       "         [ 1,  1,  1, ...,  2,  2,  2],\n",
       "         [ 1,  1,  1, ...,  1,  1,  1],\n",
       "         [ 1,  1,  1, ...,  2,  2,  2]])>),\n",
       " 'prices': (<tf.Tensor: shape=(32, 99), dtype=float64, numpy=\n",
       "  array([[ 1.        ,  1.00507606,  1.00507606, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 1.        ,  1.0064356 ,  1.00940587, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 0.75238096,  0.62857142,  0.66666667, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         ...,\n",
       "         [ 1.17991604,  1.18354644,  1.18200356, ...,  1.20916538,\n",
       "           1.19359672,  1.19359672],\n",
       "         [ 0.98748727,  0.98168868,  0.98474063, ...,  1.00406917,\n",
       "           1.00406917,  1.00508649],\n",
       "         [ 1.00412371,  1.00412371,  1.00412371, ...,  1.01752578,\n",
       "           1.01752578,  1.01649483]])>,\n",
       "  <tf.Tensor: shape=(32, 99), dtype=float64, numpy=\n",
       "  array([[ 0.94069261,  0.94424241,  0.94495238, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 1.4158045 ,  1.39032921,  1.38233116, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 0.59120002,  0.57519999,  0.596     , ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         ...,\n",
       "         [ 0.74703256,  0.73822089,  0.70101611, ...,  1.63015617,\n",
       "           1.7123983 ,  1.73002164],\n",
       "         [ 0.42722223,  0.42666668,  0.44000001, ...,  0.23333333,\n",
       "           0.22333334,  0.22333334],\n",
       "         [ 0.88206061,  0.81337554,  0.78942612, ...,  0.58743789,\n",
       "           0.60506104,  0.597831  ]])>,\n",
       "  <tf.Tensor: shape=(32, 99), dtype=float64, numpy=\n",
       "  array([[ 0.96752029,  0.95393712,  0.9433902 , ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 0.76436784,  0.83908048,  0.8448276 , ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         [ 1.        ,  1.00406091,  1.00507606, ..., -1.        ,\n",
       "          -1.        , -1.        ],\n",
       "         ...,\n",
       "         [ 0.68374997,  0.70937502,  0.70437503, ...,  0.62812501,\n",
       "           0.645625  ,  0.62812501],\n",
       "         [ 3.1562498 ,  3.12499988,  3.18749991, ...,  2.68020818,\n",
       "           2.68958319,  2.61041651],\n",
       "         [ 1.34946058,  1.35988647,  1.34007724, ...,  1.31401246,\n",
       "           1.27821674,  1.28064945]])>),\n",
       " 'tickers': <tf.Tensor: shape=(32, 3), dtype=string, numpy=\n",
       " array([[b'ADRT.U', b'GHLD', b'RAYD'],\n",
       "        [b'HGER', b'NULC', b'AKTX'],\n",
       "        [b'ALLR', b'CETX', b'AOGO'],\n",
       "        [b'ANEB', b'IBDD', b'IHD'],\n",
       "        [b'ORGS', b'ROSE', b'MAAX'],\n",
       "        [b'PSNL', b'PRTH', b'NR'],\n",
       "        [b'CLM', b'SGOV', b'EMGC'],\n",
       "        [b'JHEM', b'MTB', b'STZ.B'],\n",
       "        [b'IQSI', b'NJAN', b'IMTE'],\n",
       "        [b'CXW', b'BVS', b'TYDE'],\n",
       "        [b'ECC', b'PRLH', b'HTGC'],\n",
       "        [b'IHD', b'UTRN', b'ACHV'],\n",
       "        [b'AKA', b'CLOE', b'ESHY'],\n",
       "        [b'EMHY', b'LABP', b'SISI'],\n",
       "        [b'ATCOL', b'IHE', b'DVND'],\n",
       "        [b'GAINN', b'ARYE', b'DCP'],\n",
       "        [b'PRDO', b'LESL', b'GSQD'],\n",
       "        [b'DGII', b'JD', b'ATRO'],\n",
       "        [b'SIFI', b'FIHD', b'LOPP'],\n",
       "        [b'ACON', b'ZUO', b'URNM'],\n",
       "        [b'IRAAU', b'IMGN', b'E'],\n",
       "        [b'IQV', b'SHIP', b'LHAA'],\n",
       "        [b'PAG', b'AEPPZ', b'IWN'],\n",
       "        [b'MDWD', b'IAG', b'CBNK'],\n",
       "        [b'PTRB', b'CLX', b'SITC'],\n",
       "        [b'BOXD', b'CGABL', b'BNNR'],\n",
       "        [b'FHB', b'TGT', b'OSS'],\n",
       "        [b'ISUN', b'RNLC', b'LITE'],\n",
       "        [b'SPB', b'FLNG', b'DGHI'],\n",
       "        [b'TANNZ', b'PTEN', b'NABL'],\n",
       "        [b'LLAP', b'BRQS', b'AADI'],\n",
       "        [b'EDNC', b'FWRG', b'NURE']], dtype=object)>}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cut23(x):\n",
    "    return {\"time\":x[:4],\"prices\":x[4:7],\"tickers\":x[7]}\n",
    "\n",
    "cut23(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c133231",
   "metadata": {},
   "source": [
    "# validation dataset \n",
    "on infrance the best results would probably be achived by runing on every time entry with at least some memory of the past\n",
    "\n",
    "that means there would need to be some precomputation which we can make diffrent for timesetep. \n",
    "\n",
    "this is rediclously slow and combersom to write so our validation is just gona look mostly like the training. \n",
    "after choosing the stocks we would want to work with we are going to think of test behivior \n",
    "\n",
    "we may want to just train a new model from scratch only on that subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8fe61765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7759685516357422\n"
     ]
    }
   ],
   "source": [
    "def get_val_numbers(tickers):\n",
    "    base=choose_stocks_df(tickers,test_df)\n",
    "    \n",
    "    y=[s.iloc[1:,0].to_numpy() for s in base]\n",
    "    y=tf.ragged.constant(y)\n",
    "    y=(y+1).to_tensor()-1\n",
    "    \n",
    "    x=[s[:-1] for s in base]\n",
    "    x=get_xs(x)\n",
    "    #x.append(tickers)\n",
    "    x.append(y)\n",
    "    return x\n",
    "t=time.time()\n",
    "n=get_val_numbers(exmple)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1e716b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to do training and test set (test set should look diffrent)\n",
    "val_dataset=dataset.shuffle(1000).batch(32).map(lambda y: tf.py_function(get_val_numbers,inp=[y],Tout=specs))\n",
    "\n",
    "dataset_x=val_dataset.map(gather_x)\n",
    "dataset_y=val_dataset.map(gather_y)\n",
    "val_dataset=tf.data.Dataset.zip((dataset_x,dataset_y))\n",
    "\n",
    "x=val_dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8f7ad18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorSpec(shape=(32, 67), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(32, 67), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(32, 67), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(32, 67), dtype=tf.int32, name=None),\n",
       " TensorSpec(shape=(32, 67), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(32, 67), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(32, 67), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(32, 3), dtype=tf.string, name=None)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf.TensorSpec.from_tensor(x) for x in x[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8256ba",
   "metadata": {},
   "source": [
    "# the model\n",
    "\n",
    "**the dataset is dependent on the model!!!** this means we have some considerations on how it should look like:\n",
    "\n",
    "* there has to be an attention mechanisem on embeding space. (using only the sampling embedings dot product) \n",
    "* embedings convergnce and stabilety greatly effects the models training. (we may want to consider reverse regularization)\n",
    "*  at least some part of the model should be seprable by stock, in order to reduce noise from data sampeling\n",
    "* it would defintly help if we add residual conections to the embedings to alow gradients to flow to the embedings\n",
    "\n",
    "*side note: the padding used is -1, seems like it should propgate to the loss*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "af84e506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 32) dtype=float32 (created by layer 'tf.concat_27')>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting up the inputs\n",
    "inputs=[layers.Input([None,],dtype=x.dtype) for x in specs[:-2]]\n",
    "inputs.append(layers.Input(specs[-2].shape[1:],dtype=specs[-2].dtype,name=\"tickers\"))\n",
    "\n",
    "#time encoding\n",
    "time_depndent=[tf.expand_dims(x,-1) for x in inputs[:-1]] \n",
    "time_depndent=[layers.Masking(-1)(x) for x in time_depndent] \n",
    "\n",
    "\n",
    "day_embeding=layers.Embedding(7,7)(time_depndent[0])\n",
    "year_embeding=layers.Embedding(3,5)(time_depndent[3]) \n",
    "\n",
    "day_embeding=layers.TimeDistributed(layers.Reshape([7]))(day_embeding)\n",
    "year_embeding=layers.TimeDistributed(layers.Reshape([5]))(year_embeding)\n",
    "\n",
    "t=tf.concat(time_depndent[1:3],-1)\n",
    "t=layers.Dense(20)(t) \n",
    "\n",
    "time_encode=tf.concat([day_embeding,t,year_embeding],-1)\n",
    "time_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4f8a9a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 3, 1) dtype=float32 (created by layer 'tf.nn.softmax_3')>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers=look(inputs[-1])\n",
    "tickers=tick_embed(tickers)\n",
    "origin=tf.expand_dims(tickers[:,0],-2)\n",
    "origin=layers.Permute([2,1])(origin)\n",
    "attention=tickers@origin\n",
    "attention=tf.nn.softmax(attention)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6121d6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, None, 16) dtype=float32 (created by layer 'gru_11')>,\n",
       " <KerasTensor: shape=(None, None, 16) dtype=float32 (created by layer 'gru_11')>,\n",
       " <KerasTensor: shape=(None, None, 16) dtype=float32 (created by layer 'gru_11')>]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"need to add stock conditioning\"\n",
    "prices=time_depndent[4:7]\n",
    "s=[tf.concat([time_encode,x],-1) for x in prices]\n",
    "\n",
    "stock_layer=layers.LSTM(16,return_sequences=True)\n",
    "s=[stock_layer(x) for x in s]\n",
    "stock_layer=layers.GRU(16,return_sequences=True)\n",
    "s=[stock_layer(x) for x in s]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "dc4d326b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 3, 3, 16) dtype=float32 (created by layer 'tf.math.multiply_1')>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention*prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5613a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_layer=layers.LSTM(16,return_sequences=True) \n",
    "prices=[price_layer(x) for x in prices]\n",
    "\n",
    "price_layer=layers.GRU(16,return_sequences=True) \n",
    "prices=[price_layer(x) for x in prices]\n",
    "\n",
    "prices=[tf.expand_dims(x,1) for x in prices]\n",
    "prices=tf.concat(prices,1)\n",
    "prices\n",
    "\n",
    "del(price_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b69c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
